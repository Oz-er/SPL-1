PCA vs. Domain Adaptation (TCA, BDA, CORAL)


A Conceptual and Mathematical Comparison

1. What is PCA (Principal Component Analysis)?

PCA is a 100-year-old unsupervised machine learning algorithm used for dimensionality reduction and feature extraction.

The Goal: To take a dataset with many features (e.g., 20 software metrics) and compress it down to a smaller number of features (e.g., 5) without losing the core information.

How it works (Variance): It looks at a single dataset and finds the "Principal Components"â€”the mathematical angles (axes) where the data is the most spread out. It assumes that the direction with the widest spread (highest variance) contains the most valuable information.

The Math: It calculates the Covariance Matrix of a single dataset, finds the Eigenvectors, and projects the data onto those new axes.

2. The Core Limitation of PCA for Your Project
PCA is a "Single-Domain" algorithm. It assumes that your Training data (Apache) and your Testing data (Zxing) come from the exact same mathematical distribution.

If you apply PCA to Cross-Project Defect Prediction (CPDP), it will find the best angle to view the Apache data. However, because Zxing was written by different developers, with different coding styles, its data cloud has a completely different shape and location. PCA will compress the data, but the Apache and Zxing clouds will remain completely separated in the new latent space, causing any classifier (like K-NN) to fail.

3. What Your Algorithms Do Differently
Your algorithms belong to a field called Domain Adaptation. They do not just compress data; they mathematically force two different datasets to overlap so a classifier trained on one will work on the other.

A. TCA (Transfer Component Analysis)
The Philosophy: "Let's do PCA, but add a penalty."

The Difference: TCA specifically looks at two datasets simultaneously. It tries to find a projection that preserves the internal structure of the data (just like PCA), BUT it adds a strict condition: the new projection must also minimize the Maximum Mean Discrepancy (MMD) between the Source and Target datasets.

The Result: It compresses the data while physically dragging the global center of the Apache cloud to perfectly overlap with the global center of the Zxing cloud.

B. BDA (Balanced Distribution Adaptation)
The Philosophy: "TCA is too blind. We need to align the specific classes, too."

The Difference: Where TCA only aligns the global centers, BDA uses predicted pseudo-labels to split the clouds into sub-groups. It calculates a Marginal matrix (global) and Conditional matrices (class-specific).

The Result: It not only pulls the datasets together, but it actively rotates the space so that the Apache Bugs land exactly on top of the Zxing Bugs, preventing class mismatch. It uses a loop to refine this over 10 iterations.

C. CORAL (Correlation Alignment)
The Philosophy: "Stop pulling the centers. Let's fix the shape."

The Difference: CORAL abandons the complex Kernel tricks, MMD distances, and Eigen-decompositions used in TCA and BDA. Instead of trying to find a shared latent space, it operates entirely using Second-Order Statistics (Covariance).

The Result: It takes the Source data, strips away its feature correlations (Whitening), and then injects the Target data's feature correlations into it (Coloring). It mathematically stretches and squashes the Apache dataset until it perfectly mimics the architectural shape of the Zxing dataset.
